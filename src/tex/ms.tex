% Define document class
\documentclass[modern]{aastex631}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{showyourwork}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\Neff}{N_{\mathrm{eff}}}

\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\var}{var}

% Begin!
\begin{document}

% Title
\title{How Many Samples Do We Need?}

% Author list
\author[0000-0003-1540-8562]{Will M. Farr}
\email{will.farr@stonybrook.edu}
\affiliation{Department of Physics and Astronomy, Stony Brook University, Stony Brook, NY 11794, USA}
\email{wfarr@flatironinstitute.org}
\affiliation{Center for Computational Astrophysics, Flatiron Institute, New York, NY 10010, USA}

% Abstract with filler text
\begin{abstract}
    Lorem ipsum dolor sit amet, consectetuer adipiscing elit.
    Ut purus elit, vestibulum ut, placerat ac, adipiscing vitae, felis.
    Curabitur dictum gravida mauris, consectetuer id, vulputate a, magna.
    Donec vehicula augue eu neque, morbi tristique senectus et netus et.
    Mauris ut leo, cras viverra metus rhoncus sem, nulla et lectus vestibulum.
    Phasellus eu tellus sit amet tortor gravida placerat.
    Integer sapien est, iaculis in, pretium quis, viverra ac, nunc.
    Praesent eget sem vel leo ultrices bibendum.
    Aenean faucibus, morbi dolor nulla, malesuada eu, pulvinar at, mollis ac.
    Curabitur auctor semper nulla donec varius orci eget risus.
    Duis nibh mi, congue eu, accumsan eleifend, sagittis quis, diam.
    Duis eget orci sit amet orci dignissim rutrum.
\end{abstract}

% Main body with filler text
\section{Introduction}
\label{sec:intro}

The form of the hierarchical marginal likelihood is 
\begin{equation}
    \log \mathcal{L}\left( \mathbf{d} \mid \lambda \right) = \sum_{i=1}^{N} \log \int \dd \theta_i \, p\left( d_i \mid \theta_i \right) p\left( \theta_i \mid \lambda \right).
\end{equation}
We often approximate the integrals in this expression using Monte Carlo
integration over samples for each event $i$, $\theta_{i}^{(s)}$, $s = 1, \ldots,
S_i$, drawn from a posterior density using some fiducial prior, $p\left( \theta
\right)$:
\begin{equation}
    \log \mathcal{L}\left( \mathbf{d} \mid \lambda \right) \approx \sum_{i=1}^{N} \log \frac{1}{S_i} \sum_{s=1}^{S_i} \frac{p\left( \theta_i^{(s)} \mid \lambda \right)}{p\left( \theta_i^{(s)} \right)} .
\end{equation}
Each term in the sum takes the form of 
\begin{equation}
    \log \sum_{s=1}^{S_i} w_i^{(s)} + \mathrm{const},
\end{equation}
with 
\begin{equation}
    w_i^{(s)} = \frac{p\left( \theta_i^{(s)} \mid \lambda \right)}{p\left( \theta_i^{(s)} \right)}.
\end{equation}

The Monte-Carlo sum introduces some variance into the estimate of the log
likelihood.  The variance of each term is given by 
\begin{equation}
    \var \left( \log \sum_{s=1}^{S_i} w_i^{(s)} \right) \simeq \frac{\var\left( \sum_{s=1}^{S_i} w_i^{(s)} \right)}{\left( \sum_{s=1}^{S_i} w_i^{(s)} \right)^2} = \frac{S_i \var\left( w_i \right)}{\left( \sum_{s=1}^{S_i} w_i^{(s)} \right)^2}.
\end{equation}
The total variance in the log-likelihood is a sum of the variances for each
term.

If we define the ``effective number of samples'' $\Neff$ as the inverse of the
variance of the log of the sum of the weights, then we can estimate $\Neff$
using the empirical variance and the sum of the weights $w_i$ via%
\footnote{Equation \eqref{eq:Neff} is invariant under rescaling of the weights
by a constant factor; a convenient choice of rescaling can help with numerical
stability in the estimate of $\Neff$.  Usually we have access to the log of the
weights (to avoid under- or overflow).  Let $l_s = \log w_i^{(s)}$ (suppressing
the $i$ index for clarity).  Then let $\mu_l = \log \sum_s \exp l_s$ be the log
of the sum of the weights; $\mu_l$ is computable without over- or underflow via
the well-known \texttt{logsumexp} special function.  Then let $\tilde{w}_i^{(s)}
= \exp\left( w_i^{(s)} - \mu_l \right)$; the $\tilde{w}_i^{(s)}$ sum to 1, and
therefore their variance can be computed without undue roundoff error.  With
this rescaling, $\Neff = 1 / \left( S \var\left( \tilde{w}_i \right) \right)$.}%
\begin{equation}
    \label{eq:Neff}
    \Neff = \frac{\left( \sum_{s=1}^{S_i} w_i^{(s)} \right)^2}{S_i \var\left( w_i \right)}.
\end{equation}
Note that in the limit that the sum of the weights is dominated by a single
weight, say (without loss of generality) the $s = 1$ weight, then we have 
\begin{equation}
    \sum_{s=1}^{S_i} w_i^{(s)} \simeq w_i^{(1)},
\end{equation}
and (assuming $S_i \gg 1$) 
\begin{equation}
    S_i \var\left( w_i \right) \simeq \left( w_i^{(1)} \right)^2,    
\end{equation}
so that $\Neff \simeq 1$.  There is no upper bound on $\Neff$ (equal weights
would have $\Neff \to \infty$); but suppose that some small subset of the
weights, $s = 1, \ldots, N_i$ with $1 \ll N_i \ll S_i$ contribute about equally
to the sum of the weights, and all other contributions are negligable.  Then 
\begin{equation}
    \sum_{s=1}^{S_i} w_i^{(s)} \simeq N_s w_i^{(1)},
\end{equation}
and 
\begin{equation}
    S_i \var\left( w_i \right) \simeq N_s \left( w_i^{(1)} \right)^2,
\end{equation}
and $\Neff \simeq N_s$, as would be expected.

We demonstrate here with a simple hierarchical statistical model that a
sufficient condition on the accuracy of the Monte-Carlo estimate of the the
marginal log likelihood is that each term in the sum has $\Neff \gg 1$ (here we
use a threshold of 10), even if the total variance of the marginal log
likelihood is larger than 1.  We argue that this condition should be sufficient
for other, more complex hierarchical models because it does not depend on the
\emph{structure} of the likelihood function; in some sense, as long as many
samples contribute to the Monte-Carlo estimate of the log-likelihood for each
observation, the model ``knows'' about the degree of uncertainty in that
observation, and can ``take it into account.''

Our hierarchical model is a simple ``normal-normal'' two-level hierarchy.
Suppose we have a population of $N$ objects with parameter $x$, and that the
population distribution is normal with mean $\mu$ and s.d.\ $\sigma$:
\begin{equation}
    x_i \sim \mathcal{N}\left( \mu, \sigma \right) \quad i = 1, \ldots, N.
\end{equation}
Each object is observed and its $x$ parameter is measured with some (known)
normal uncertainty $\sigma_i$ and no bias to be $x_{\mathrm{obs},i}$:
\begin{equation}
    x_{\mathrm{obs},i} \sim \mathcal{N}\left( x_i, \sigma_i \right) \quad i = 1, \ldots, N.
\end{equation}

A hierarchical model for estimating the population parameters $\mu$ and
$\sigma$ would have a likelihood of the form 
\begin{equation}
    \log \mathcal{L}\left( \mathbf{x}_{\mathrm{obs}} \mid \mu, \sigma, \boldsymbol{\sigma} \right) = \sum_{i=1}^{N} \log \int \dd x_i \, \mathcal{N}\left( x_{\mathrm{obs},i} \mid x_i, \sigma_i \right) \mathcal{N}\left( x_i \mid \mu, \sigma \right),
\end{equation}
where we are using $\mathbf{x}_\mathrm{obs}$ and $\boldsymbol{\sigma}$ to
represent the complete set of observations and their uncertainties.  This model
is simple enough that we can compute the exact marginal likelihood via a
Gaussian integral; it is 
\begin{equation}
    \log \mathcal{L}\left( \mathbf{x}_{\mathrm{obs}} \mid \mu, \sigma, \boldsymbol{\sigma} \right) = \sum_{i=1}^N \log \mathcal{N}\left( x_{\mathrm{obs},i} \mid \mu, \sqrt{\sigma^2 + \sigma_i^2} \right).
\end{equation}
But we can also draw $S_i$ samples from a fiducial ``posterior'' for $x_i$, $p_i\left( x_i \mid x_{\mathrm{obs},i}, \sigma_i \right)$, and use Monte Carlo integration to estimate the marginal likelihood:
\begin{equation}
    \log \mathcal{L}\left( \mathbf{x}_{\mathrm{obs}} \mid \mu, \sigma, \boldsymbol{\sigma} \right) \simeq \sum_{i=1}^N \log \frac{1}{S_i} \sum_{s=1}^{S_i} \frac{\mathcal{N}\left( x_i^{(s)} \mid \mu, \sigma \right)}{p_i\left( x_i^{(s)} \mid x_{\mathrm{obs},i}, \sigma_i \right)}.
\end{equation}

We have simulated $N = 128$ observations from this model with population
parameters $\mu = 0$, $\sigma = 1$.  Each observation has a randomly-chosen
$\sigma_i \sim \mathcal{U}(1, 2)$, with $x_{\mathrm{obs},i}$ drawn accordingly.
We impose broad priors, $\mathcal{N}(0,1)$ on $\mu$ and $\Exp(1)$ on $\sigma$.
We then use the No-U-Turn Sampler (NUTS) \citep{Hoffman2011} variant of
Hamiltonian Monte-Carlo \citep{Neal2011} as implemented in the
\texttt{Turing.jl} package \citep{Ge2018} to draw samples of $\mu$ and $\sigma$
from the exact and approximate posteriors.  After sampling, we compute the
effective number of samples for each observation's likelihood using Eq.\
\eqref{eq:Neff}, and re-run the MCMC after drawing additional samples for any
observation that has $\Neff < 10$.  Ultimately, our final MCMC sampling has
$\Neff = \variable{output/min_neff.txt}$ for the observation with the smallest
$\Neff$ and total log-likelihood variance $\var\left( \log \mathcal{L} \right) =
\variable{output/max_lp_var.txt}$. The results are shown in Figure
\ref{fig:mu-sigma}; it is apparent that the sampling from the approximate
likelihood has converged to that of the exact likelihood \emph{even though the
total variance of the log-likelihood is larger than 1}.  

\begin{figure}
    \includegraphics[width=\columnwidth]{figures/mu_sigma_pairplot.pdf}
    \script{plot_mu_sigma.jl}
    \caption{\label{fig:mu-sigma} The posterior density for the parameters $\mu$
    and $\sigma$ in the hierarchical model.  The sampling using the exact
    marginal likelihood is shown in blue; the sampling from the approximate
    Monte-Carlo likelihood once all observations have $\Neff > 10$ (the minimum
    $\Neff = \variable{output/min_neff.txt}$ for this sampling) is shown in
    orange. Over all posterior samples in the converged Monte-Carlo sampling,
    the largest total variance for the marginal log likelihood was
    \variable{output/max_lp_var.txt}.  It is apparent that the sampling from the
    approximate likelihood has converged to that of the exact likelihood even
    though the total variance of the log-likelihood is considerably larger than
    1.}
\end{figure}

\bibliography{bib}

\end{document}
